{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2022985",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b565504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import heapq\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d13ea",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_author(string: str, two_layer=False):\n",
    "    # Remove name of chat user\n",
    "    index = string.find(':')  # Find the index of the first colon\n",
    "    if index != -1:\n",
    "        string = string[index + 1:]\n",
    "    return string if not two_layer else remove_author(string, two_layer=False)\n",
    "\n",
    "def process_chat_message(string: str, two_layer=False, process_msg=True):\n",
    "    # Remove name of chat user\n",
    "    if process_msg:\n",
    "        string = remove_author(string, two_layer=two_layer)\n",
    "    # Encode string\n",
    "    return model.encode(string)\n",
    "\n",
    "def add_training_embeddings(df):\n",
    "    embeddings = df.Input.progress_apply(process_chat_message, two_layer=True)\n",
    "    df['embedding'] = embeddings\n",
    "    return df\n",
    "\n",
    "def get_trained_centroids(df, cats):\n",
    "    d_avg = dict()\n",
    "    for cat in cats:\n",
    "        d_avg[cat] = df[df[cat]==1].copy()['embedding'].mean()\n",
    "    return d_avg\n",
    "\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    # Convert the vectors to 2D arrays\n",
    "    vector1 = np.array(vector1).reshape(1, -1)\n",
    "    vector2 = np.array(vector2).reshape(1, -1)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = cosine_similarity(vector1, vector2)\n",
    "\n",
    "    return similarity[0, 0]\n",
    "\n",
    "def get_most_similar_category(emb: np.array, d_avg: dict):\n",
    "    d_tmp = {k: calculate_cosine_similarity(emb, v) for k, v in d_avg.items()}\n",
    "    return max(d_tmp, key=d_tmp.get)\n",
    "\n",
    "def eval_classification(df):\n",
    "    true_labels = df['label']\n",
    "    predicted_labels = df['pred']\n",
    "    report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    acc = accuracy_score(true_labels, predicted_labels)\n",
    "    return report, matrix, acc\n",
    "\n",
    "def get_test_folds(df, n_splits=10):\n",
    "    split_data = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)  # Set shuffle=True for random shuffling of data\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        df_train = df.iloc[train_index].copy()\n",
    "        df_test = df.iloc[test_index].copy()\n",
    "        split_data.append((df_train, df_test))\n",
    "    return split_data #tuple of train, test\n",
    "\n",
    "def run_cv(df_train, df_test, cats):\n",
    "    d_avg = get_trained_centroids(df_train, cats)\n",
    "    df_test['pred'] = df_test.embedding.map(lambda emb: get_most_similar_category(emb, d_avg))\n",
    "    return eval_classification(df_test)\n",
    "\n",
    "def get_avg_dict(d_dict):\n",
    "    average_dict = {key: np.mean([d[key] for d in d_dict]) for key in d_dict[0].keys()}\n",
    "    return average_dict\n",
    "\n",
    "def get_all_avg_dicts(reports, cats):\n",
    "    ans = {cat: get_avg_dict([d[cat] for d in reports]) for cat in cats}\n",
    "    return ans\n",
    "\n",
    "def str_to_array(s):\n",
    "    return np.array(s.split(','))\n",
    "\n",
    "def create_chats(d_tx, two_layer=False):\n",
    "    d_tx = d_tx.sort_values(by = ['Session Id', 'Time'])\n",
    "    d_chats = d_tx[\n",
    "        (d_tx['Action'] == 'sendMessage') &\n",
    "        (d_tx['Student Response Subtype'] != 'tutor-performed') &\n",
    "        (d_tx['Input'].map(lambda s: False if pd.isna(s) else s[-9:] != '_computer')) # System-performed messages\n",
    "    ][['Session Id', 'Transaction Id', 'Anon Student Id', 'Input']].copy().reset_index()\n",
    "    del d_chats['index']\n",
    "    d_chats['chat_msg'] = d_chats.Input.map(lambda s: remove_author(s, two_layer=two_layer))\n",
    "    return d_chats\n",
    "\n",
    "def proc_s(input_string):\n",
    "    # Convert to lowercase\n",
    "    preprocessed_string = input_string.lower()\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    preprocessed_string = preprocessed_string.strip()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    preprocessed_string = re.sub(r'[^\\w\\s]', '', preprocessed_string)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    preprocessed_string = re.sub(r'\\s+', ' ', preprocessed_string)\n",
    "    \n",
    "    return preprocessed_string\n",
    "\n",
    "# Fetch context window of k=2, i.e., up to 1,2,INPUT,4,5 \n",
    "def add_joined_input(df):\n",
    "    \"\"\"\n",
    "    Creates a context window of chats for prediction.\n",
    "    \"\"\"\n",
    "    joined_input = []\n",
    "    for string in df['Input']:\n",
    "        search_s = proc_s(string)\n",
    "\n",
    "        # Find the index where search_s exists in Input_clean\n",
    "        search_index = d_chats[d_chats['Input_clean'].str.contains(search_s, na=False)].index[0]\n",
    "\n",
    "        # Get the session id at the search index\n",
    "        session_id = d_chats.at[search_index, 'Session Id']\n",
    "\n",
    "        # Filter the rows based on the session id\n",
    "        session_rows = d_chats[d_chats['Session Id'] == session_id]\n",
    "\n",
    "        # Find the index of the search_s within the session rows\n",
    "        search_index_in_session = session_rows[session_rows['Input_clean'].str.contains(search_s, na=False)].index[0]\n",
    "\n",
    "        # Get the indices for the previous and next rows\n",
    "        prev_indices = session_rows.iloc[max(0, search_index_in_session - 2):search_index_in_session].index\n",
    "        next_indices = session_rows.iloc[search_index_in_session + 1:min(search_index_in_session + 3, len(session_rows))].index\n",
    "\n",
    "        # Concatenate the indices for previous, current, and next rows\n",
    "        indices_to_fetch = pd.Index(list(prev_indices) + [search_index_in_session] + list(next_indices))\n",
    "\n",
    "        # Fetch the chat messages using the indices\n",
    "        chat_messages = d_chats.loc[indices_to_fetch, 'chat_msg'].tolist()\n",
    "\n",
    "        # Prepare the final string with tokens\n",
    "        final_string = ''\n",
    "        for i, message in enumerate(chat_messages):\n",
    "            index_token = '[SEP]'\n",
    "            final_string += f'{message} {index_token} '\n",
    "\n",
    "        # Add START and END tokens\n",
    "        final_string = '[CLS] ' + final_string.strip()\n",
    "        joined_input.append(final_string)\n",
    "\n",
    "    df['Input'] = joined_input\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_joined_input_full(df):\n",
    "    \"\"\"\n",
    "    Generate analogous context windows on the full chat data set\n",
    "    to which the trained model is applied.\n",
    "    \"\"\"\n",
    "    joined_input = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Get the session id at the search index\n",
    "        session_id = df.at[index, 'Session Id']\n",
    "\n",
    "        # Filter the rows based on the session id\n",
    "        session_rows = df[df['Session Id'] == session_id]\n",
    "\n",
    "        # Get the indices for the previous and next rows\n",
    "        prev_indices = session_rows.iloc[max(0, index - 2):index].index\n",
    "        next_indices = session_rows.iloc[index + 1:min(index + 3, len(session_rows))].index\n",
    "\n",
    "        # Concatenate the indices for previous, current, and next rows\n",
    "        indices_to_fetch = pd.Index(list(prev_indices) + [index] + list(next_indices))\n",
    "\n",
    "        # Fetch the chat messages using the indices\n",
    "        chat_messages = df.loc[indices_to_fetch, 'chat_msg_clean'].tolist()\n",
    "\n",
    "        # Prepare the final string with tokens\n",
    "        final_string = ''\n",
    "        for i, message in enumerate(chat_messages):\n",
    "            index_token = '[SEP]'\n",
    "            final_string += f'{message} {index_token} '\n",
    "\n",
    "        # Add START and END tokens\n",
    "        final_string = '[CLS] ' + final_string.strip()\n",
    "        joined_input.append(final_string)\n",
    "\n",
    "    df['Input_scale'] = joined_input\n",
    "\n",
    "    return df\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "def train_bert_classifier(model=None):\n",
    "    \"\"\"\n",
    "    If a model is passed into this function, \n",
    "    then the validation set is directly returned for evaluation.\n",
    "    \"\"\"\n",
    "    bert_model_name = 'bert-base-uncased'\n",
    "    num_classes = 3\n",
    "    max_length = 128*2\n",
    "    batch_size = 16\n",
    "    num_epochs = 4\n",
    "    learning_rate = 5e-5\n",
    "\n",
    "    d_labels = {'Minimal Participation':0,\n",
    "              'Facilitative participation':1,\n",
    "              'Constructive Participation':2\n",
    "              }\n",
    "\n",
    "    texts = df_bert.Input.to_list()\n",
    "    labels = df_bert.label.map(d_labels).to_list()\n",
    "\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    if model is None:\n",
    "        model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_dataloader) * num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            train(model, train_dataloader, optimizer, scheduler, device)\n",
    "            accuracy, report = evaluate(model, val_dataloader, device)\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "            print(report)\n",
    "\n",
    "        torch.save(model.state_dict(), \"bert_classifier-2.pth\")\n",
    "        return val_dataloader, device\n",
    "    else:\n",
    "        return val_dataloader, device\n",
    "\n",
    "def predict_label(text, model, tokenizer, device, max_length=128*2):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aed83d",
   "metadata": {},
   "source": [
    "## Reading in Tutor Log Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3832994",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b205b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tx_1 = pd.read_csv('data/logdataschool1.txt', \n",
    "                      sep='\\t', low_memory=False)\n",
    "d_chats_1 = create_chats(d_tx_1, two_layer=True)\n",
    "\n",
    "d_tx_2 = pd.read_csv('data/logdataschool2.txt', \n",
    "                      sep='\\t', low_memory=False)\n",
    "d_chats_2 = create_chats(d_tx_2, two_layer=False)\n",
    "\n",
    "d_tx_3 = pd.read_csv('logdataschool3.txt', \n",
    "                      sep='\\t', low_memory=False)\n",
    "d_chats_3 = create_chats(d_tx_3, two_layer=False)\n",
    "\n",
    "d_tx = pd.concat([d_tx_1, d_tx_2, d_tx_3])\n",
    "d_chats = pd.concat([d_chats_1, d_chats_2, d_chats_3])\n",
    "\n",
    "d_chats = d_chats[d_chats['chat_msg'] != ''].copy()\n",
    "\n",
    "d_chats['chat_msg_clean'] = d_chats['chat_msg'].map(proc_s)\n",
    "\n",
    "d_chats['Input_clean'] = d_chats['Input'].map(proc_s)\n",
    "\n",
    "d_chats = d_chats.reset_index(); del d_chats['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44074fc9",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b25973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cleaned-apta-codings.csv')\n",
    "df.dropna(subset=['Input'], inplace=True) # Remove empty messages\n",
    "df.fillna(0, inplace=True)\n",
    "for c in df.columns:\n",
    "    if c != 'Input':\n",
    "        df[c] = df[c].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e121596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_joined_input(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_level_categories = [\n",
    "    'Minimal Participation',\n",
    "    'Facilitative participation',\n",
    "    'Constructive Participation'\n",
    "]\n",
    "for cat in high_level_categories:\n",
    "    tmp = df[[c for c in df.columns if cat in c]].select_dtypes(include='number').copy()\n",
    "    l = tmp.sum(axis=1).to_list()\n",
    "    df[cat] = [1 if x>=1 else 0 for x in l]\n",
    "    \n",
    "# Remove ambiguous and unclassified labels\n",
    "df = df[df[high_level_categories].sum(axis=1) == 1].copy()\n",
    "\n",
    "# Add label\n",
    "df['label'] = df[high_level_categories].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Classifier\n",
    "df_bert = df[['Input', 'label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, report = train_bert_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2887e1",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier('bert-base-uncased', 3)\n",
    "model.load_state_dict(torch.load(\"bert_classifier-2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b13847",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader, device = train_bert_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aceef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, cohen_kappa_score\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    probabilities = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            probabilities.extend(torch.softmax(outputs, dim=1).cpu().tolist())\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    probabilities = np.array(probabilities)\n",
    "    auc = roc_auc_score(actual_labels, probabilities, multi_class='ovr', average='weighted')\n",
    "    \n",
    "    # Calculate Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(actual_labels, predictions)\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(actual_labels, predictions)\n",
    "    \n",
    "    return accuracy, auc, kappa, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, auc, kappa, class_report = evaluate(model, val_dataloader, device)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"AUC:\", auc)\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c347a0",
   "metadata": {},
   "source": [
    "## Apply Model to all Chat Messags in the Data Set and Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41054c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = BERTClassifier('bert-base-uncased', 3)\n",
    "m2.load_state_dict(torch.load(\"bert_classifier-2.pth\"))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "device = torch.device(\"cpu\")\n",
    "predict_label('who are you?', m2, tokenizer, device, max_length=128*2)\n",
    "\n",
    "d_chats = add_joined_input_full(d_chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_chats['predicted_label'] =\\\n",
    "    d_chats.Input_scale.progress_apply(lambda s: predict_label(s, m2, tokenizer, device, max_length=128*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_this = d_chats[['Transaction Id', 'predicted_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_this['predicted_label'] = join_this.predicted_label.map(lambda x: x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d_tx.merge(join_this, how='left', on='Transaction Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867299f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analysis-set.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
